{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce356da0",
   "metadata": {},
   "source": [
    "# SK Part 1: Basic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16710000",
   "metadata": {},
   "source": [
    "## Three Common Types of `Supervised Learning` Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca7f06",
   "metadata": {},
   "source": [
    "The three common types of target feature  t  are as follows:\n",
    "\n",
    "1. **Continuous targets**. For example, house prices; loan amounts.\n",
    "2. **Binary targets**. For instance, whether a patient has Type 2 diabetes or not; whether a loan will default or not.\n",
    "3. **Multinomial (a.k.a. multiclass) targets**. For example, five-level Likert items such as \"very poor\", \"poor\", \"average\", \"good\" and \"very good\".\n",
    "\n",
    "Let's get familiar with some terminology. When the target feature is continuous, we coin it as a \"regression problem\". The predictive model is then called a \"regressor\". If the target feature is binary or multinomial, we say it is a \"classification problem\". In fact, binary is a special case of multinomial targets (it has only two classes). The model built is called a \"classifier\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e441fa",
   "metadata": {},
   "source": [
    "## <u>Binary Classification</u> Example: Breast Cancer Wisconsin Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdcbe04",
   "metadata": {},
   "source": [
    "This dataset contains 569 observations and has 30 input features. The target feature has two classes: 212 \"malignant\" (M) and 357 \"benign\" (B)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf49e0",
   "metadata": {},
   "source": [
    "## Preparing Data for Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3206577",
   "metadata": {},
   "source": [
    "We first load the data from `sklearn` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0380f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181b0615",
   "metadata": {},
   "source": [
    "Let's scale each descriptive feature to be between 0 and 1 before fitting any classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24982f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'C:\\\\Users\\\\piyus\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\breast_cancer.csv'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our df is a tuple which have two objects \"data\" and target as we can see below\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28354235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are setting both objects to separate data set as Data and target\n",
    "Data, target = df.data, df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3db916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = preprocessing.MinMaxScaler().fit_transform(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65da980e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52103744, 0.0226581 , 0.54598853, 0.36373277, 0.59375282,\n",
       "        0.7920373 , 0.70313964, 0.73111332, 0.68636364, 0.60551811,\n",
       "        0.35614702, 0.12046941, 0.3690336 , 0.27381126, 0.15929565,\n",
       "        0.35139844, 0.13568182, 0.30062512, 0.31164518, 0.18304244,\n",
       "        0.62077552, 0.14152452, 0.66831017, 0.45069799, 0.60113584,\n",
       "        0.61929156, 0.56861022, 0.91202749, 0.59846245, 0.41886396],\n",
       "       [0.64314449, 0.27257355, 0.61578329, 0.50159067, 0.28987993,\n",
       "        0.18176799, 0.20360825, 0.34875746, 0.37979798, 0.14132266,\n",
       "        0.15643672, 0.08258929, 0.12444047, 0.12565979, 0.11938675,\n",
       "        0.08132304, 0.0469697 , 0.25383595, 0.08453875, 0.0911101 ,\n",
       "        0.60690146, 0.30357143, 0.53981772, 0.43521431, 0.34755332,\n",
       "        0.15456336, 0.19297125, 0.63917526, 0.23358959, 0.22287813],\n",
       "       [0.60149557, 0.3902604 , 0.59574321, 0.44941676, 0.51430893,\n",
       "        0.4310165 , 0.46251172, 0.63568588, 0.50959596, 0.21124684,\n",
       "        0.22962158, 0.09430251, 0.18037035, 0.16292179, 0.15083115,\n",
       "        0.2839547 , 0.09676768, 0.38984656, 0.20569032, 0.12700551,\n",
       "        0.55638563, 0.36007463, 0.50844166, 0.37450845, 0.48358978,\n",
       "        0.38537513, 0.35974441, 0.83505155, 0.40370589, 0.21343303]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data[:3,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8740549c",
   "metadata": {},
   "source": [
    "The target feature is already encoded. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183cbd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([212, 357], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(target,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e8b29",
   "metadata": {},
   "source": [
    "However, we would like \"malignant\" to be the positive class (1) and \"benign\" to be the negative class (0). So we use the \"where\" function as below to reverse the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a971a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.where(target==0,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b16eb2",
   "metadata": {},
   "source": [
    "Let's check to make sure the labels are now reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e28fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([357, 212], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(target,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec7e56",
   "metadata": {},
   "source": [
    "## Spliting Data into Training and Test Sets \n",
    "We split the descriptive features and the target feature into a training set and a test set by a ratio of 70:30. That is, we use 70 % of the data to build a classifier and evaluate its performance on the test set.\n",
    "\n",
    "To split data, we use `train_test_split` function from `sklearn`.\n",
    "\n",
    "In a classification problem, we might have an uneven proportion of classes. In the breast cancer example, the target has 212 \"M\" and 357 \"B\" classes. Therefore, when splitting the data into training and test sets, it is possible that the class proportions in these split sets might be different from the original one. So, in order to ensure the proportion is not deviating from the ratio of 212/357 when splitting the data, we set the `stratify` option in `train_test_split` function to the `target` array.\n",
    "\n",
    "Furthermore, in order to be able to replicate our analysis later on, we set the `random_state` option to 999.\n",
    "\n",
    "Finally, in order to ensure the data is split randomly, we set the `shuffle` option to \"True\" (which, by the way, is \"True\" by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81700cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "D_train, D_test, t_train, t_test = \\\n",
    "train_test_split(Data,target, test_size=0.3,\n",
    "                 stratify=target,shuffle=True,random_state=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b6664",
   "metadata": {},
   "source": [
    "## Fitting a Nearest Neighbor Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d128b0",
   "metadata": {},
   "source": [
    "Let's try a nearest neighbor classifier with 2 neighbors using the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29bfdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=2,p=2)\n",
    "#p=1 is Manhattan distance and p=2 is Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9451a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier.fit(D_train, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ddff6c",
   "metadata": {},
   "source": [
    "Done! We have created a nearest neighbor classifier. We shall use accuracy to evaluate this classifer using the test set. The accuracy metric is defined as:\n",
    "\n",
    "$\\;\\;\\;$$Accuracy = \\dfrac{Number\\; of \\: correct\\: predicted\\: labels}{Number\\: of\\: total\\: observations}$\n",
    "\n",
    "In order to evaluate the performance of our classifier on the test data, we use the score method and set `X = D_test` and `y = t_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8266417b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier.score(X=D_test, y=t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f69c1",
   "metadata": {},
   "source": [
    "The nearest neighbor classifier scores an accuracy rate of 97% in this particular case on the test data. That is impressive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befa54d",
   "metadata": {},
   "source": [
    "## Fitting a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b23f0",
   "metadata": {},
   "source": [
    "Let's say we want to fit a decision tree with a maximum depth of 4 (`max_depth = 4`) using information gain for split criterion (`criterion = 'entropy'`). For reproducibility, we set `random_state = 999`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6668c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=4, criterion = 'entropy',\n",
    "                                       random_state=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db340dad",
   "metadata": {},
   "source": [
    "Now lets fit the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "129cecfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=999)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_classifier.fit(D_train,t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5c2d222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415204678362573"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_classifier.score(D_test,t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b134d",
   "metadata": {},
   "source": [
    "The decision tree predicts the correct labels on the test set with an accuracy rate of 94%. However, there are other performance metrics, such as precision, recall, and F1 score, to assess model performance from different angles. That we will acheive in our future practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b74920",
   "metadata": {},
   "source": [
    "## Fitting a Gaussian Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf655280",
   "metadata": {},
   "source": [
    "One last model we would like to fit to the breast cancer dataset is the Gaussian Naive Bayes classifier with a variance smoothing value of  $10^−3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ea0070c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9532163742690059"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_classifier = GaussianNB(var_smoothing=10**(-3))\n",
    "nb_classifier.fit(D_train,t_train)\n",
    "nb_classifier.score(D_test,t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b513e282",
   "metadata": {},
   "source": [
    "We observe that the accuracy of the Gaussian Naive Bayes and decision tree classifiers are slightly lower compared to that of the nearest neighbor classifier.\n",
    "\n",
    "We would have to perform multiple runs in a cross-validation setting and then conduct a \"paired t-test\" in order to determine if this difference is statistically significant or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705dfb54",
   "metadata": {},
   "source": [
    "# Regression Example: Boston Housing Data\n",
    "## <u>(Continuous targets)</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa74c5",
   "metadata": {},
   "source": [
    "## Reading and Spliting Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464501ac",
   "metadata": {},
   "source": [
    "The Boston Housing Data is available within `sklearn` datasets. Let's load the dataset and use 70 % of the data for training and the remaining 30 % for testing. The goal is to build a decision tree regressor to predict _median value_ of owner-occupied homes in thousand dollars (labeled as `MEDV`) in Boston in 1970's. The input data has been cleaned; in particular, `CHAS` (Charles River dummy variable = 1 if tract bounds river; 0 otherwise) is already encoded. To display more information, you can print `housing_df.DESCR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "354b072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "housing_data = load_boston()\n",
    "print(housing_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbb1d556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n",
       "        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02,\n",
       "        1.5300e+01, 3.9690e+02, 4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9690e+02, 9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9283e+02, 4.0300e+00]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data.data[:3,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7928e3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data.target[:3,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71cfad17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "D_train, D_test, t_train, t_test = \\\n",
    "    train_test_split(housing_data.data, housing_data.target, test_size = 0.3,\n",
    "        shuffle=True, random_state=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b748b5c",
   "metadata": {},
   "source": [
    "## Fitting and Evaluating a Regressor \n",
    "We create a decision tree regressor object (`DecisionTreeRegressor`) with a maximum depth of 4. _Since it is a regression problem, we cannot build the model using accuracy. Instead, we build the regressor based on `mean squared error (MSE)` performance metric_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da584676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=4, random_state=999)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_regressor = DecisionTreeRegressor(max_depth=4, random_state=999)\n",
    "dt_regressor.fit(D_train,t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a12d1c",
   "metadata": {},
   "source": [
    "To compute MSE, we first need to predict on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc17f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pred = dt_regressor.predict(D_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70461dc3",
   "metadata": {},
   "source": [
    "Next, we import `mean_squared_error` from `sklearn.metrics` module and compute MSE using the predicted and test target feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffce456d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1718934750451577"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mse = mean_absolute_error(t_test, t_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d218a4c",
   "metadata": {},
   "source": [
    "It is more intuitive to examine the root of MSE, which is denoted by RMSE, rather than MSE itself as RMSE is in the same units as the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f9679b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7809810428651838"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c72957f",
   "metadata": {},
   "source": [
    "We observe that our decision tree regressor achieves a RMSE value of 1.78 (thousand dollars) for the Boston housing dataset."
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
